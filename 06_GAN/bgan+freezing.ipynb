{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UDCPvHVVJpgd"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3KcN8CRhCfT"},"outputs":[],"source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhPH4x4vuH5R"},"outputs":[],"source":["# !pip install update torch --extra-index-url https://download.pytorch.org/whl/cu113\n","!pip uninstall torch -y\n","!pip uninstall torchvision -y\n","!pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RUOU-wOXkWCo"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader,random_split\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from kobert import get_tokenizer\n","from kobert import get_pytorch_kobert_model\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","from PIL import Image\n","import torchvision.transforms as T\n","import pandas as pd\n","import torchvision.utils as vutils\n","\n","device = 'cuda'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PiHd6mFEk6Ox"},"outputs":[],"source":["class BERTBcdata(Dataset):\n","    def __init__(self, csv_path, img_path, tok, max_seq_length, img_shape=64):\n","        self.transform = nlp.data.BERTSentenceTransform(tok, max_seq_length=max_seq_length, pad=True, pair=False)\n","        self.df = pd.read_csv(csv_path).dropna()\n","        self.img_path = img_path\n","        self.img_shape = img_shape\n","        self.transpose = T.Compose([\n","            T.Resize(img_shape+8),\n","            T.CenterCrop(img_shape),\n","            T.ToTensor(),\n","            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","        ])\n","\n","    def __getitem__(self, i):\n","        sentences = self.transform([str(self.df.iloc[i]['comment'])])\n","        label = int(self.df.iloc[i]['Emotion'])\n","        img_name = '{}/{}.jpg'.format(self.img_path, label)\n","        img = Image.open(img_name)\n","        label = self.transpose(img)\n","\n","        return sentences[0], sentences[1], sentences[2], label\n","\n","    def __len__(self):\n","        return (len(self.df))\n","\n","\n","class BERTBcTestdata(Dataset):\n","    def __init__(self, csv_path, img_path, tok, max_seq_length, img_shape=64):\n","        self.transform = nlp.data.BERTSentenceTransform(tok, max_seq_length=max_seq_length, pad=True, pair=False)\n","        self.df = pd.read_csv(csv_path).dropna()\n","       \n","\n","    def __getitem__(self, i):\n","        sentences = self.transform([str(self.df.iloc[i]['comment'])])\n","        return sentences[0], sentences[1], sentences[2]\n","\n","    def __len__(self):\n","        return (len(self.df))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTMFhzJ6ubbk"},"outputs":[],"source":["bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x8CagiG5qjwN"},"outputs":[],"source":["dataset = BERTBcdata('/content/drive/Shareddrives/선형대수학/주제분석/emoji/취합1.csv',\n","           '/content/drive/Shareddrives/선형대수학/주제분석/emoji',\n","           tok,\n","           100)\n","len(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eLc9W78muuhD"},"outputs":[],"source":["data_size = len(dataset)\n","val_size = 16\n","\n","train_size = data_size - val_size\n","train_dataset, val_dataset =  random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n","len(train_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xaYbqUO7wz-B"},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size = 8, shuffle = True)\n","test_loader = DataLoader(val_dataset, batch_size = 16)\n","\n","for data in train_loader:\n","    print(data[0].shape) \n","    print(data[1].shape) \n","    print(data[2].shape) \n","    print(data[3].shape)\n","    \n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_RD8kbyGxlKz"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, nc=3, ndf=8):\n","        super(Discriminator, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)    \n","        self.lkr1 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. (ndf) x 32 x 32       3 x 32 x 32\n","        self.conv2 = nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)\n","        self.bm2 = nn.BatchNorm2d(ndf * 2)\n","        self.lkr2 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. (ndf*2) x 16 x 16\n","        self.conv3 = nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)\n","        self.bm3 = nn.BatchNorm2d(ndf * 4)\n","        self.lkr3 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. (ndf*4) x 8 x 8\n","        self.conv4 = nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)\n","        self.bm4 = nn.BatchNorm2d(ndf * 8)\n","        self.lkr4 = nn.LeakyReLU(0.2, inplace=True)\n","        # state size. (ndf*8) x 4 x 4\n","        self.conv5 = nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False)\n","        self.sig = nn.Sigmoid()\n","\n","        self.dropout = nn.Dropout(0.25)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.lkr1(x)\n","        x = self.conv2(x)\n","        x = self.bm2(x)\n","        x = self.lkr2(x)\n","        x = self.conv3(x)\n","        x = self.bm3(x)\n","        x = self.lkr3(x)\n","        x = self.conv4(x)\n","        x = self.bm4(x)\n","        x = self.lkr4(x)\n","\n","        # x = self.dropout(x)\n","        \n","        x = self.conv5(x)\n","        x = self.sig(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSt1kwBZyh3P"},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self, nz, ngf, nc):\n","        super(Generator, self).__init__()\n","        self.main = nn.Sequential(\n","            # input is Z, going into a convolution\n","            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.ReLU(True),\n","            # state size. (ngf*8) x 4 x 4\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.ReLU(True),\n","            # state size. (ngf*4) x 8 x 8\n","            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.ReLU(True),\n","            # state size. (ngf*2) x 16 x 16\n","            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf),\n","            nn.ReLU(True),\n","            # state size. (ngf) x 32 x 32\n","            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n","            nn.Tanh()\n","            # state size. (nc) x 64 x 64\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZSarbLrWylSk"},"outputs":[],"source":["class BGenerator(nn.Module):\n","    def __init__(self, bertmodel, bert_dim=768, ngf=8, nc=3, device='cuda',freezing_layer_num = 11):\n","        super(BGenerator, self).__init__()\n","        self.bertmodel = bertmodel.to(device)\n","        for n,p in bertmodel.named_parameters():\n","            print(n)\n","            if 'embeddings' in n:\n","                p.requires_grad= False\n","\n","            if 'encoder.layer' in n:\n","                layer_num = n.split(sep ='.')[2] \n","                if int(layer_num) <= freezing_layer_num:\n","                    p.requires_grad = False\n","        self.generator = Generator(bert_dim, ngf, nc).to(device)\n","\n","        self.bertmodel.named_parameters()\n","\n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        _, cls = bertmodel(input_ids=token_ids, token_type_ids=segment_ids.long(),\n","                        attention_mask=attention_mask.float().to(token_ids.device))\n","        \n","        x = self.generator(cls.view(-1,768,1,1))\n","\n","        return x\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TqDAI0YF2GdA"},"outputs":[],"source":["bg = BGenerator(bertmodel, device = device)\n","d = Discriminator().to(device)\n","# bg.load_state_dict(torch.load('path'))\n","# d.load_state_dict(torch.load('path'))\n","criterion = nn.BCELoss()\n","optimizerD = optim.Adam(d.parameters(), lr=0.0002)\n","optimizerG = optim.Adam(bg.parameters(),lr = 0.0002)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQibZcsnhnyu"},"outputs":[],"source":["img_list = []\n","G_losses = []\n","D_losses = []\n","img_list1 = []\n","\n","for e in range(3000):\n","\n","    trainD_loss = 0\n","    trainG_loss = 0\n","    bg.train()\n","    d.train()\n","    for data in train_loader:\n","        \n","        d.zero_grad()\n","        \n","        fake_img = bg(data[0].to(device),data[1].to(device),data[2].to(device)) \n","\n","        fake_result = d(fake_img).view(-1)  \n","        label = torch.zeros(fake_result.size()).to(device)\n","        fakeD_loss = criterion(fake_result, label)  \n","\n","\n","        true_result = d(data[3].to(device)).view(-1)    \n","        true_label = torch.ones(true_result.size()).to(device)  \n","        trueD_loss = criterion(true_result, true_label) \n","        \n","\n","        fakeD_loss.backward(retain_graph=True)\n","        trueD_loss.backward(retain_graph=True)\n","\n","        optimizerD.step()\n","\n","        trainD_loss += fakeD_loss.item() \n","        trainD_loss += trueD_loss.item()    \n","    \n","        bg.zero_grad()\n","\n","        fake_result = d(fake_img).view(-1)  \n","        fake_label = torch.ones(fake_result.size()).to(device)\n","        fakeG_loss = criterion(fake_result, fake_label)\n","        fakeG_loss.backward()\n","\n","        optimizerG.step()\n","\n","        trainG_loss += fakeG_loss.item()\n","\n","        G_losses.append(trainD_loss)\n","        D_losses.append(trainG_loss)\n","\n","\n","    print('{}epoch trainD_loss:{}'.format(e+1, trainD_loss/len(train_loader)))\n","    print('{}epoch trainG_loss:{}'.format(e+1, trainG_loss/len(train_loader)))\n","    bg.eval()\n","\n","    if (e+1)%10 == 0:\n","        for data in test_loader:\n","            fake_img = bg(data[0].to(device), data[1].to(device), data[2].to(device)).detach().cpu()\n","            img_list1.append(fake_img)\n","            img_list.append(vutils.make_grid(fake_img, padding=2, normalize=True))\n","        torch.save(bg.state_dict(),'./bg_{}.pt'.format(e+1))    \n","        torch.save(d.state_dict(),'./d_{}.pt'.format(e+1))   \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXrar6wnmbwU"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","row = len(img_list1)\n","col = len(img_list1[0])\n","fig, axes = plt.subplots( row, col, figsize = (5*col,5*row))\n","print(row)\n","print(col)\n","for i in range(row):\n","    for j in range(col):\n","        img = np.transpose(img_list1[i][j], (1,2,0))\n","        axes[i][j].imshow(img)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLgeB8tc2agT"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","plt.plot(G_losses,label=\"G\")\n","plt.plot(D_losses,label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JXZmbi73wQnd"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","\n","fig = plt.figure(figsize=(8,8))\n","plt.axis(\"off\")\n","ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n","ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n","\n","HTML(ani.to_jshtml())"]},{"cell_type":"code","source":["!pip install imagemagick"],"metadata":{"id":"8FWh1nJTe3mz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPDRX7e-vgjf"},"outputs":[],"source":["anim.save('sine_wave_interval_100ms.gif', writer='imagemagick')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"private_outputs":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}