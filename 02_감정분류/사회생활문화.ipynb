{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install gluonnlp pandas tqdm   \n","!pip install mxnet\n","!pip install sentencepiece==0.1.91\n","!pip install transformers==4.8.1\n","!pip install torch\n","!pip3 install pickle5\n","import pickle5 as pickle"],"metadata":{"id":"jrcysDj3iuoT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"],"metadata":{"id":"agevJrBHi0od"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"metadata":{"id":"Rf90B4TejQ1V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from kobert.pytorch_kobert import get_pytorch_kobert_model\n","from kobert_tokenizer import KoBERTTokenizer\n","tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","#  bertmodel, vocab = get_kobert_model('skt/kobert-base-v1',tokenizer.vocab_file)"],"metadata":{"id":"rml49-UKjXdV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","import pandas as pd\n","\n","#transformers\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","from transformers import BertModel\n","\n","#GPU 사용 시\n","# device = torch.device(\"cuda:0\")\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"],"metadata":{"id":"hNVibAfQmY5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_kobert_model(model_path, vocab_file, ctx=\"cpu\"):\n","    bertmodel = BertModel.from_pretrained(model_path)\n","    device = torch.device(ctx)\n","    bertmodel.to(device)\n","    bertmodel.eval()\n","    vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(vocab_file,\n","                                                         padding_token='[PAD]')\n","    return bertmodel, vocab_b_obj\n","import gluonnlp as nlp\n","bertmodel, vocab = get_kobert_model('skt/kobert-base-v1',tokenizer.vocab_file)"],"metadata":{"id":"HBrqZ2eGp-w-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n","                 pad, pair):\n","   \n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n","        \n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","         \n","\n","    def __len__(self):\n","        return (len(self.labels))"],"metadata":{"id":"62DqCDGVpjca"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setting parameters\n","max_len = 64\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 5  \n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5"],"metadata":{"id":"bQLIMtRqpm9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes=7,   ##클래스 수 조정##\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict=False)\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)\n","#BERT 모델 불러오기\n","model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n"," \n","#optimizer와 schedule 설정\n","# no_decay = ['bias', 'LayerNorm.weight']\n","# optimizer_grouped_parameters = [\n","#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","# ]\n","\n","# optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","# loss_fn = nn.CrossEntropyLoss() # 다중분류를 위한 대표적인 loss func\n","\n","# t_total = len(train_dataloader) * num_epochs\n","# warmup_step = int(t_total * warmup_ratio)\n","\n","# scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n","\n","# #정확도 측정을 위한 함수 정의\n","# def calc_accuracy(X,Y):\n","#     max_vals, max_indices = torch.max(X, 1)\n","#     train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","#     return train_acc\n","    \n","# train_dataloader"],"metadata":{"id":"zjJxn3k_tFkZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"eL9JvA8iBoUy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PATH = 'drive/MyDrive/'\n","model = torch.load('/content/drive/Shareddrives/선형대수학/주제분석/감정분류모델/KoBERT_감정분류.pt')  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n","# model.load_state_dict(torch.load('model_state_dict.pt'))"],"metadata":{"id":"d7oVCuxtRxgU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(),'model_state_dict.pt')"],"metadata":{"id":"fqsci5s4SbAu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load('/content/model_state_dict.pt'))"],"metadata":{"id":"bY2DbwiHSjHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","\n","#transformers\n","from transformers import AdamW # 인공지능 모델의 초기값 지정 함수를 아담으로 지정한다.\n","from transformers.optimization import get_cosine_schedule_with_warmup"],"metadata":{"id":"7p_NJiWcRc7V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"],"metadata":{"id":"uUXduhOURW9K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_history=[]\n","# test_history=[]\n","# loss_history=[]\n","# for e in range(num_epochs):\n","#     train_acc = 0.0\n","#     test_acc = 0.0\n","#     model.train()\n","#     for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","#         optimizer.zero_grad()\n","#         token_ids = token_ids.long().to(device)\n","#         segment_ids = segment_ids.long().to(device)\n","#         valid_length= valid_length\n","#         label = label.long().to(device)\n","#         out = model(token_ids, valid_length, segment_ids)\n","         \n","#         #print(label.shape,out.shape)\n","#         loss = loss_fn(out, label)\n","#         loss.backward()\n","#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","#         optimizer.step()\n","#         scheduler.step()  # Update learning rate schedule\n","#         train_acc += calc_accuracy(out, label)\n","#         if batch_id % log_interval == 0:\n","#             print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","#             train_history.append(train_acc / (batch_id+1))\n","#             loss_history.append(loss.data.cpu().numpy())\n","#     print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","#     #train_history.append(train_acc / (batch_id+1))\n","    \n","#     model.eval()\n","#     for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","#         token_ids = token_ids.long().to(device)\n","#         segment_ids = segment_ids.long().to(device)\n","#         valid_length= valid_length\n","#         label = label.long().to(device)\n","#         out = model(token_ids, valid_length, segment_ids)\n","#         test_acc += calc_accuracy(out, label)\n","#     print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n","#     test_history.append(test_acc / (batch_id+1))"],"metadata":{"id":"1n-dVlsvtK9q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(predict_sentence):\n","\n","    data = [predict_sentence, '0']\n","    dataset_another = [data]\n","\n","    another_test = BERTDataset(dataset_another, 0, 1, tok, vocab, max_len, True, False)\n","    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n","    \n","    model.eval()\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","\n","        out = model(token_ids, valid_length, segment_ids)\n","\n","\n","        test_eval=[]\n","        for i in out:\n","            logits=i\n","            logits = logits.detach().cpu().numpy()\n","\n","            if np.argmax(logits) == 0:\n","                test_eval.append(\"공포\")\n","            elif np.argmax(logits) == 1:\n","                test_eval.append(\"놀람\")\n","            elif np.argmax(logits) == 2:\n","                test_eval.append(\"분노\")\n","            elif np.argmax(logits) == 3:\n","                test_eval.append(\"슬픔\")\n","            elif np.argmax(logits) == 4:\n","                test_eval.append(\"중립\")\n","            elif np.argmax(logits) == 5:\n","                test_eval.append(\"행복\")\n","            elif np.argmax(logits) == 6:\n","                test_eval.append(\"혐오\")\n","\n","        return(test_eval[0])"],"metadata":{"id":"lRKv1677tTqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv('/content/drive/Shareddrives/선형대수학/주제분석/DTW+클러스터링/데이터/생활문화_1월.csv',encoding = 'UTF-8-sig')\n","#################\n","##여기바꿔주세요#\n","#################\n","data"],"metadata":{"id":"SozpENlzSyo1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = data.sample(5000).reset_index()"],"metadata":{"id":"GSyWxdgoRtkY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pol_df = pd.DataFrame(index=range(0,5000), columns=['comment', 'emotion'])\n","for i in range(5000):\n","  a=predict(data['comment'][i])\n","  b = data['comment'][i]\n","  pol_df['comment'][i] = b\n","  pol_df['emotion'][i] = a\n","  print(i)"],"metadata":{"id":"0-hkSYPlS8io"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pol_df"],"metadata":{"id":"1rW8Bd43Ri5H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle # 파일 저장 여기 바꿔!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!11\n","\n","pol_df.to_pickle(\"/content/drive/Shareddrives/선형대수학/주제분석/DTW+클러스터링/감정분류/감정_생활문화_1월.pkl\")  ###########파일이름!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","pol_df['emotion'].value_counts()"],"metadata":{"id":"0LCo1T9BTAIe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pol_df.to_csv('/content/drive/Shareddrives/선형대수학/주제분석/DTW+클러스터링/감정분류/감정_생활문화_1월.csv', encoding = 'UTF-8-sig', index=False)"],"metadata":{"id":"7gTypc1zjOX6"},"execution_count":null,"outputs":[]}]}